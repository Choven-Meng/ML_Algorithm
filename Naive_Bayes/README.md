朴素贝叶斯是使用概率论来分类的算法(选取概率大的情况进行分类)。其中 **朴素** ：各特征条件独立；**贝叶斯**：根据贝叶斯定理。     
当要根据多个特征而非一个特征对数据进行分类的时候，我们可以假设这些特征相互独立（或者先假设相互独立），然后利用条件概率乘
法法则得到每一个分类的概率， 然后选择概率最大的那个作为机器的判定。    

**朴素** 的见解：之所以叫朴素贝叶斯，因为它简单、易于操作，基于特征独立性假设，假设各个特征不会相互影响，这样就大大减小了计算概率的难度。  

贝叶斯条件概率公式：  
  **P(A|B) = P(AB)/P(B) = P(B|A)P(A)/P(B)**

## 朴素贝叶斯的参数估计

1、极大似然估计

2、贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为0的情况，解决这一问题的方法是采用贝叶斯估计。  


朴素贝叶斯最常见的分类应用是对文档进行分类，因此，最常见的特征条件是文档中，出现词汇的情况，通常将词汇出现的特征条件用词向量 ω表示，由多个数值组成，
数值的个数和训练样本集中的词汇表个数相同。

### 估计类别下特征属性划分的条件概率及Laplace校准

 当特征属性为连续值时，通常假定其值服从高斯分布（也称正态分布）。即：  
 <img title="g(x,\eta ,\sigma )=\frac{1}{\sqrt{2\pi }\sigma }e^-\frac{(x-\eta)^2}{2\sigma^2}" src="http://latex.codecogs.com/gif.latex?g%28x,%5Ceta%20,%5Csigma%20%29=%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%20%7D%5Csigma%20%7De%5E-%5Cfrac%7B%28x-%5Ceta%29%5E2%7D%7B2%5Csigma%5E2%7D" alt="">

 因此只要计算出训练样本中各个类别中此特征项划分的各均值和标准差，代入上述公式即可得到需要的估计值。

 另一个需要讨论的问题就是当P(a|y)=0怎么办，当某个类别下某个特征项划分没有出现时，就是产生这种现象，这会令分类器质量大大降低。为了解决这个问题，我们引入Laplace校准，它的思想非常简单，就是对没类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述频率为0的尴尬局面。




**优点：**

1、朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。  
2、对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。  
3、对缺失数据不太敏感，算法也比较简单，常用于文本分类。  

**缺点：**

1、理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中
往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之
类的算法通过考虑部分关联性适度改进。  
2、需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。  
3、由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。对输入数据的表达形式很敏感。  
