决策树（decision tree）是一种基本的分类与回归方法。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是if-then规则的集合，
也可以认为是定义在特征空间与类空间上的条件概率分布。主要优点是 **模型具有可读性，分类速度快**。**学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类**。  
决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。  

# 决策树模型与学习

## 决策树模型

**定义：** 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点(node)和有向边(directed edge)组成。节点有两种类型：内部节点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。  
用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归的对实例进行测试并分类，直至达到叶节点。最后将实例分到叶结点的类中。  
如图是一个决策树示意图，图中圆和方框分别表示内部结点和叶结点  
![决策树模型](http://images0.cnblogs.com/blog/790160/201508/281727480004448.png)  
 
 ## 决策树学习
 
 决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。    
**决策树学习的损失函数通常是 正则化的极大似然函数** 。  
**决策树学习的目标是 以损失函数为目标函数的最小化** 。    
        决策树学习的算法通常是一个 **递归的选择最优特征**，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这个特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归的进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即有了明确的类。这就生成一颗决策树。


 



