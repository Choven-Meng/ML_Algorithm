决策树（decision tree）是一种基本的分类与回归方法。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是if-then规则的集合，
也可以认为是定义在特征空间与类空间上的条件概率分布。主要优点是 **模型具有可读性，分类速度快**。**学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类**。  
决策树学习通常包括3个步骤：**特征选择、决策树的生成和决策树的修剪**。  

# 1、决策树模型与学习

## 1.1、决策树模型

**定义：** 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点(node)和有向边(directed edge)组成。节点有两种类型：内部节点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。  
用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归的对实例进行测试并分类，直至达到叶节点。最后将实例分到叶结点的类中。  
如图是一个决&emsp;&emsp;&emsp;&emsp;策树示意图，图中圆和方框分别表示内部结点和叶结点  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![决策树模型](photos/决策树模型.png)  
 
 ## 1.2、决策树学习
 
 决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。    
**决策树学习的损失函数通常是 正则化的极大似然函数** 。  
**决策树学习的目标是 以损失函数为目标函数的最小化** 。    
> 决策树学习的算法通常是一个 **递归的选择最优特征**，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这个特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归的进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即有了明确的类。这就生成一颗决策树。  
以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据可能过拟合。需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体的就是去掉过于细分的叶结点，使其退回到父结点，甚至更高的结点，然后将其改为新的叶结点。  
若特征较多，在决策树学习开始的时候，对 **特征进行选择**，只留下对训练数据有足够分类能力的特征。  
**决策树的生成只考虑局部最优**  
**决策树的剪枝只考虑全局最优**  

# 2、特征选择

特征选择在于选取**对训练数据具有分类能力**的特征，可以提高决策树学习的效率。通常特征选择的准则是**信息增益或信息增益率**。    
特征选择的划分依据：这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就应该选择这个特征。（将数据集划分为纯度更高，不确定性更小的子集的过程。）    

## 2.1、信息增益

**（1）信息熵：** 

在信息论和概率统计中，**熵(entropy)** 是表示随机变量不确定性的度量（纯度）。是用来衡量一个随机变量出现的期望值。如果信息的不确定性越大，熵的值也就越大，出现的各种情况也就越多。
假设随机变量X的可能取值有x<sub>1，</sub>x<sub>2...,</sub>x<sub>n</sub>,对于每一个可能的取值x<sub>i</sub>，其概率 P(X=x<sub>i</sub>) = p<sub>i</sub> , ( i = 1,2, ... , n)  
因此随机变量X的熵：  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;![](https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170323162933986-429761079.png)  
由定义可知，熵只依赖于X的分布，而与X的取值无关。  
对于样本集合D来说，随机变量X是样本的类别，即，假设样本有k个类别，每个类别的概率是<img width="27" height="38" alt="" src="https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317131605495-1565634841.png">，其中|C<sub>k</sub>|表示类别k的样本个数，|D|表示样本总数，则对于样本集合D来说熵（经验熵）为：  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img width="200" height="57" alt="" src="https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317154810807-1726620815.png">  

**（2）条件熵**

假设有随机变量(X,Y)，其联合概率分布为:P(X=x<sub>i</sub>,Y=y<sub>j</sub>)=p<sub>ij</sub>,i=1,2,⋯,n;j=1,2,⋯,m  
则条件熵(H(Y∣X))表示在已知随机变量X的条件下随机变量Y的不确定性，其定义为X在给定条件下Y的条件概率分布的熵对X的数学期望:  
<img alt="" src="https://img-blog.csdn.net/20141214191555257">  
当嫡和条件嫡中的概率由数据估计(特别是极大似然估计)得到时，所对应的嫡与条件嫡分别称为经验熵( empirical entropy)和经验条件嫡(empirical conditional entropy )。  

**信息增益：**

**定义：** 特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即:  
 &emsp;   &emsp; &emsp; &emsp; &emsp;  **g(D,A)=H(D)-H(D|A)**  
划分前样本集合D的熵是一定的 ，entroy(前)，使用某个特征A划分数据集D，计算划分后的数据子集的熵 entroy(后):      
 &emsp; &emsp; &emsp; &emsp;  **信息增益 =  entroy(前) -  entroy(后)**    

信息增益(information gain)表示由于特征A使得对数据集D的分类的不确定性减少的程度。    
根据信息增益准则的特征选择方法是：对训练数据集D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。



**信息增益算法：**  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img width="335" height="145"  src="http://images0.cnblogs.com/blog/790160/201508/281728022508401.png" border="0">    

**缺点：** 信息增益偏向取值较多的特征

**原因：** 当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较 偏向取值较多的特征。

<img src="photos/信息增益1.png" width="200" height="200" >  <img src="photos/信息增益2.png" width="256" height="200" >

**解决方法：** 使用信息增益率(information gain ratio)

## 2.2、信息增益率   

<div align="center"><strong>信息增益率 = 惩罚参数 * 信息增益</strong></div>     

**书中公式：**

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img width="137" height="54" alt="" src="https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317133854323-1046650322.png">
 
**注意：** 其中的H<sub>A</sub>(D)，对于样本集合D，将当前特征A作为随机变量（取值是特征A的各个特征值），求得的经验熵。  
之前是把集合类别作为随机变量，现在把某个特征作为随机变量，按照此特征的特征取值对集合D进行划分，计算熵H<sub>A</sub>(D)  
<img width="216" height="65" alt="" src="https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317135209885-3830592.png">  
**信息增益比本质：** 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。  
**惩罚参数：** 数据集D以特征A作为随机变量的熵的倒数，即：将特征A取值相同的样本划分到同一个子集中（之前所说数据集的熵是依据类别进行划分的）。  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img width="283" height="63" alt="" src="https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317132527166-2071609059.png"> 

**缺点：** 信息增益比偏向取值较少的特征

**原因：** 当特征取值较少时HA(D)的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征

**使用信息增益比：** 基于以上缺点，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息
增益率最高的特征。
 

# 3、决策树的生成

## 3.1、ID3算法

&emsp;&emsp;ID3算法的核心是在决策树各个节点上应用 **信息增益** 准则选择特征，递归的构建决策树。具体方法是：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归的调用以上方法，构建决策树；知道所以特征的信息增益均很小或没有特征可以选择为止。ID3相当于用极大似然法进行概率模型的选择。  
&emsp;&emsp;ID3算法的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有4种取值，那么数据将被切分成4份。一旦按照某特征切分后，该特征在之后的算法执行过程中将不会再起作用；另一种方法是二元切分法，即每次把数据集切分成两份。如果数据的某特征值等于切分所要求的值，那么这些数据将进入左子树，反之则进入右子树。  
&emsp;&emsp;除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接出来连续型特征。只有事先将连续型特征转为离散型，才能在ID3中使用。 但这种转换过程会破坏连续型变量的内在性质。而使用 **二元切分法** 则易于对树构建过程中进行调整以处理连续型特征。  
&emsp;&emsp; ** 具体的处理方法是：**    
如果特征值大于给定值就走左子树，否则走右子树。另外二元切分法也节省了树的构建时间。

## 3.2、C4.5算法

C4.5算法用信息增益率选择特征。

# 4、决策树的剪枝

&emsp;&emsp;决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往**对训练数据的分类很准确，但对未知的测试数据的分类却没那么准确，即出现过拟合现象**。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出**过于复杂**的决策树。解决这个问题的办法是考虑决策树的**复杂度**，对已生成的决策树进行简化。   
&emsp;&emsp; **在决策树学习中将已生成的树进行简化的过程称为剪枝(pruning)** 。具体的，剪枝从已生成的树上裁掉一些字树或叶结点，并将其根节点或父节点作为新的叶结点，从而简化分类树的模型。  
**训练集分类越准确，决策树的复杂度越高。训练集分类模糊，决策树的复杂度就相对比较低。**  
&emsp;&emsp;先来看看书中关于损失函数的定义。决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。设树T的叶结点个数为|T|，t是树T的叶结点，该叶结点有
N<sub>t</sub>个样本点，其中k类样本点有N<sub>tk</sub>个，k=1,2,...,K.H<sub>t</sub>(T)为叶结点t上的经验熵，α≥0为参数，则决策树学习的损失函数可以定义为：  
&emsp;&emsp;&emsp;&emsp;<img width="136" height="29"   src="http://images0.cnblogs.com/blog/790160/201508/281728114373709.png" >
&emsp;&emsp;&emsp;&emsp;<img width="153" height="42"   src="http://images0.cnblogs.com/blog/790160/201508/281728121567095.png" >  
&emsp;&emsp;&emsp;&emsp;<img width="177" height="34"   src="http://images0.cnblogs.com/blog/790160/201508/281728127347438.png" >
&emsp;&emsp;&emsp;&emsp;<img width="136" height="29"    src="http://images0.cnblogs.com/blog/790160/201508/281728133758567.png" >  

&emsp;&emsp;C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示平莫型复杂度，参数a>=0控制两者之间的影响。剪枝，就是当a确定时，选择损失函数最小的模型，即损失函数最小的子树。损失函数正好表示了对模型的复杂度和训练数据的拟合两者的平衡  

决策树生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合，学习局部的模型；

决策树剪枝通过优化损失函数还考虑了减小模型复杂度，学习整体的模型。

利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。  

> 在这里我存在两个疑问：
> * 叶结点上为什么会有有限个N<sub>t</sub>个样本点？且为何存在k类样本点？即在该叶结点上，所有类别标签应该都是一致的。  
（可能解释：1.在该叶结点上没有足够的特征信息再把不一致的标签分开。2.存在噪声点，何谓噪声，在类群中有极少个与类群不符的标签。）  
> * 假如上述问题成立，那么叶结点存在的噪声是不需要做处理的，因为这种噪声天然的被大量类群所包围，而决策树生成过程中，在叶结点上取条件概率大的那一类作为该叶结点的类别标签。由此，难道在决策树生成时，某个维度上的特征是可以自动被切分的？如果此条件成立，那么上述式子区分噪声点才有了意义。  

决策树的剪枝策略最基本的有两种：预剪枝（pre-pruning）和后剪枝（post-pruning）：  
> * 预剪枝（pre-pruning）：预剪枝就是在构造决策树的过程中，先对每个结点在划分前进行估计，若果当前结点的划分不能带来决策树模型泛华性能的提升，则不对当前结点进行划分并且将当前结点标记为叶结点。  
> * 后剪枝（post-pruning）：后剪枝就是先把整颗决策树构造完毕，然后自底向上的对非叶结点进行考察，若将该结点对应的子树换为叶结点能够带来泛华性能的提升，则把该子树替换为叶结点。  

### 1、预剪枝  

根据西瓜书数据集通过信息增益可以构造出一颗未剪枝的决策树： ：  

&emsp;&emsp;&emsp;&emsp;<img title=""   width="300" height="250" alt="数据集" src="https://img-blog.csdn.net/20180208172456138">      &emsp;&emsp;&emsp;&emsp;<img title=""  width="300" height="250" alt="未剪枝的决策树" src="https://img-blog.csdn.net/20180209215220372">
 
构造过程如下：

<img title="" width="800" height="1200" alt="信息增益决策树构造过程" src="https://img-blog.csdn.net/20180209163602537">

因为色泽和脐部的信息增益值最大，所以从这两个中随机挑选一个，这里选择脐部来对数据集进行划分，这会产生三个分支，如下图所示：   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img title=""  width="300" height="100"   alt="5" src="https://img-blog.csdn.net/20180209204009350">

&emsp;&emsp;但是因为是预剪枝，所以要判断是否应该进行这个划分，判断的标准就是 **看划分前后的泛华性能是否有提升，也就是如果划分后泛华性能有提升，则划分；否则，不划分**。 下面来看看是否要用 **脐部** 进行划分，划分前：所有样本都在根节点，把该结点标记为叶结点，其类别标记为训练集中样本数量最多的类别，因此标记为好瓜，然后用验证集对其性能评估，可以看出样本{4，5，8}被正确分类，其他被错误分类，因此精度为43.9%。**划分后：** 划分后的的决策树为：   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img title=""  width="300" height="100" alt="决策树" src="https://img-blog.csdn.net/20180209211000815">

则验证集在这颗决策树上的精度为：5/7 = 71.4% > 42.9%。因此，用 **脐部** 进行划分。 
&emsp;&emsp;接下来，决策树算法对结点 (2) 进行划分，再次使用信息增益挑选出值最大的那个特征，这里我就不算了，计算方法和上面类似，信息增益值最大的那个特征是“色泽”，则使用“色泽”划分后决策树为：   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img title="" width="300" height="100" alt="7" src="https://img-blog.csdn.net/20180209223121754">

&emsp;&emsp;但到底该不该划分这个结点，还是要用验证集进行计算，可以看到划分后，精度为：5/7=0.571<0.714，因此，预剪枝策略将禁止划分结点 (2) 。对于结点 (3) 最优的属性为“根蒂”，划分后验证集精度仍为71.4%，因此这个划分不能提升验证集精度，所以预剪枝将禁止结点 (3) 划分。对于结点 (4) ，其所含训练样本已属于同一类，所以不再进行划分。  
&emsp;&emsp;所以基于预剪枝策略生成的最终的决策树为：  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img title=""  width="300" height="200" alt="最终的决策树" src="https://img-blog.csdn.net/20180209230048241">  

> **总结：** 对比未剪枝的决策树和经过预剪枝的决策树可以看出：预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。但是，另一方面，因为预剪枝是基于“贪心”的，所以，虽然当前划分不能提升泛华性能，但是基于该划分的后续划分却有可能导致性能提升，因此预剪枝决策树有可能带来欠拟合的风险。


### 2、后剪枝 

&emsp;&emsp;后剪枝就是先构造一颗完整的决策树，然后自底向上的对非叶结点进行考察，若将该结点对应的子树换为叶结点能够带来泛华性能的提升，则把该子树替换为叶结点。  
&emsp;&emsp;后剪枝算法首先考察上图中的结点6（纹理），若将以其为根节点的子树删除，即相当于把结点 (6) 替换为叶结点，替换后的叶结点包括编号为{7,15}的训练样本，因此把该叶结点标记为“好瓜”（因为这里正负样本数量相等，所以随便标记一个类别），因此此时的决策树在验证集上的精度为57.1%（为剪枝的决策树为42.9%），所以后剪枝策略决定剪枝，剪枝后的决策树如下图所示：   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img title="" alt="后剪枝1" width="300" height="200" src="https://img-blog.csdn.net/20180211103012158">

 &emsp;&emsp;接着考察结点 5，同样的操作，把以其为根节点的子树替换为叶结点，替换后的叶结点包含编号为{6,7,15}的训练样本，根据“多数原则”把该叶结点标记为“好瓜”，测试的决策树精度认仍为57.1%，所以不进行剪枝。  
 &emsp;&emsp;考察结点 2 ，和上述操作一样，不多说了，叶结点包含编号为{1,2,3,14}的训练样本，标记为“好瓜”，此时决策树在验证集上的精度为71.4%，因此，后剪枝策略决定剪枝。剪枝后的决策树为：  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img title="" width="300" height="200" alt="后剪枝2" src="https://img-blog.csdn.net/2018021110384118">

&emsp;&emsp;接着考察结点 3 ，同样的操作，剪枝后的决策树在验证集上的精度为71.4%，没有提升，因此不剪枝；对于结点 1 ，剪枝后的决策树的精度为42.9%，精度下降，因此也不剪枝。  
&emsp;&emsp;因此，基于后剪枝策略生成的最终的决策树如上图所示，其在验证集上的精度为71.4%。

> **总结：** 对比预剪枝和后剪枝，能够发现，后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛华性能往往也要优于预剪枝决策树。但后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，因此其训练时间开销要比未剪枝决策树和预剪枝决策树都大得多。  

**剪枝算法:**  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img width="422" height="197" title="剪枝算法"  alt="剪枝算法" src="http://images0.cnblogs.com/blog/790160/201508/281728140629411.jpg" >


# 5、CART算法

分类与回归树(classification and regression tree, CART)既可以用于回归也可以用于分类。使用二元切分法来处理连续型数值。   
&emsp;&emsp;CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是输入给定的条件下输出的条件概率分布。   

**CART算法由以下两步组成：** 

 > 1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；  
 > 2. 决策树剪枝：用验证数据集最已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。
 
 ## 5.1、CART生成  
 
 &emsp;&emsp;CART 树的生成就是递归地构建二叉决策树的过程。**对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则** ，进行特征选择，生成二叉树。  
 
 #### （1）回归树的生成
 
 &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; <img width="388" height="298" alt="回归树" src="http://images0.cnblogs.com/blog/790160/201508/281728156254454.png" border="0">
 
 ### （2）分类树的生成
 
 分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。
 
 **定义（基尼指数）:**
 
 &emsp;&emsp;分类问题中，假设有K个类，样本点属于第k类的概率为p<sub>k</sub>，则概率分布的基尼指数定义为:  
 
 &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp;<img width="240" height="40"  alt="基尼指数" src="http://images0.cnblogs.com/blog/790160/201508/281728173287241.png" border="0">

对于二类分类问题，若样本点属于第1个类的概率是p，则概率分布的基尼指数为 :  
 &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp;Gini(p)=2p(1-p)  
 
 对于给定的样本集合D，其基尼指数为 :  
  &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp;<img width="149" height="40"  src="http://images0.cnblogs.com/blog/790160/201508/281728180006613.png" border="0">  
  C<sub>k</sub>是D中属于第k类的样本子集，K是类的个数。  
  
  如果样本集合D根据特征A是否取某一可能值a被分割成D<sub>1</sub>和D<sub>2</sub>两部分，即:  
  &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp;<img width="240" height="30" src="http://images0.cnblogs.com/blog/790160/201508/281728203593002.png" border="0">
  
  则在特征A的条件下，集合D的基尼指数定义为:  
 &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; <img width="240" height="50" src="http://images0.cnblogs.com/blog/790160/201508/281728187034228.png" border="0">
 
 基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D,A)表示A =a 分割后集合D的不确定性。基尼指数值越大，样本集合的不确定性也越大，这一点与熵相似。所以选择基尼指数较小的划分。  
 
 简单来说，基尼指数反映了数据集合的混乱程度。当基尼指数越大时，当前数据越混沌的，分布均匀时取极值。
 
 **举例分析基尼指数是如何一并选择特征向量和寻找最佳切分点的：**    
 
 > 下表是一个由15个样本组成的贷款申请训练数据:  
 > <img width="300" height="250" src="https://img-blog.csdn.net/20161121195010647" alt="Alt text" title="">
 
>解：首先计算各特征的基尼指数，选择最优特征以及最优切分点。分别以A1,A2,A3,A4表示年龄、有工作、有自己的房子和信贷情况4个特征，并以1，2，3表示年龄的值为青年、中年和老年，以1，2表示有工作和有自己的房子的值是和否，以1，2，3表示信贷情况的值为非常好、好和一般。

求特征A1的基尼指数：

|      |	青年（总量 = 5）|	中年、老年（总量 = 10）|
| :---:|     :---:      |        :---:          |
| 能否贷款|	否，否，是，是，否|	否，否，是，是，是，是，是，是，是，否|

基尼指数在选取最优切分点的过程中，会分为当前特征标签和其他特征标签两类。所以 :  
&emsp;&emsp;&emsp;&emsp; Gini(D,A<sub>1</sub>=1)=(5/15){2×(2/5)×(1−(2/5)}+10/15{2×(7/10)×(1−7/10)}=0.44    

第一部分是青年标签里能否贷款的数据混沌度，第二部分是中年和老年加在一起的数据混沌度。同理：   
&emsp;&emsp;&emsp;&emsp; Gini(D,A<sub>2</sub>=1)=0.32  
&emsp;&emsp;&emsp;&emsp; Gini(D,A<sub>3</sub>=1)=0.27  
由于A2和A3只有一个切分点，所以它们就是最优切分点。  
求特征A<sub>4</sub>的基尼指数：   
&emsp;&emsp;&emsp;&emsp;Gini(D,A<sub>4</sub>=1)=0.36  
&emsp;&emsp;&emsp;&emsp;Gini(D,A<sub>4</sub>=2)=0.47    
&emsp;&emsp;&emsp;&emsp;Gini(D,A<sub>4</sub>=3)=0.32  

Gini(D,A<sub>4</sub>=3)最小，所以A<sub>4</sub>=3为A<sub>4</sub>的最优切分点。  
在A<sub>1</sub>,A<sub>2</sub>,A<sub>3</sub>,A<sub>4</sub>几个特征中，Gini(D,A<sub>3</sub>=1)=0.27最小，所以选择特征A<sub>3</sub>为最优特征，A3=1为其最优切分点。于是根结点生成两个子结点，一个是叶结点。对另一个结点继续使用以上方法在A<sub>1</sub>,A<sub>2</sub>,A<sub>4</sub>中选择最优特征及其最优切分点，结果是A<sub>2</sub>=1，以此计算得知，所得结点都是叶结点。


**总结：**  
从上述解题过程中，我们发现Gini指数不仅用来选取最优特征，还用来选择最优切分点。而在ID3和C4.5算法中，信息熵并没有用此来计算最优切分点而仅仅用来选择最优特征。当然，我们也可以自己实现信息熵的最优切分点来看看决策树的生成效果如何。

**算法（CART生成算法）：**  

>输入：训练数据集D，停止计算的条件；   
 输出：CART决策树；   
 根据训练数据集，从根节点开始，递归地对每个结点进行以下操作，构建二叉决策树：   
 （1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时对每一个特征A，对其可能取的每个值a，根据样本点对A=a的测试为“是”或“否”将D分割成D<sub>1</sub>和D<sub>2</sub>两部分，利用基尼指数计算公式计算。   
 （2）在所有可能的特征A以及它们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。  
 （3）对两个子结点递归地调用（1），（2），直至满足停止条件。  
 （4）生成CART决策树   
 算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值，或者没有更多特征。  
 
 <img src="https://pic1.zhimg.com/80/v2-b84f27024b097eb4c9245e8a04504b7a_hd.jpg" data-caption="" data-size="normal" data-rawwidth="759" data-rawheight="356"  width="759" >
 
 
 ## 连续值和缺失值处理  
 
 ##### 1、连续值
 
 采用二分法对连续属性离散化，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为<=t与>t。  
> * 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点 a <sub>i<\sub> 与 a <sub>i+1<\sub> 的均值 （n-1个，n为α所有的取值数目）。  
 > * 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。  
 > * 选择最大信息增益的划分点作为最优划分点。

##### 2、缺失值

现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：  
<img src="https://img-blog.csdn.net/20161213093945397?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" >

对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：  

<img src="https://img-blog.csdn.net/20161213093956194?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" >

对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：

<img src="https://img-blog.csdn.net/20161213094002819?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" >
