决策树（decision tree）是一种基本的分类与回归方法。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是if-then规则的集合，
也可以认为是定义在特征空间与类空间上的条件概率分布。主要优点是 **模型具有可读性，分类速度快**。**学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类**。  
决策树学习通常包括3个步骤：**特征选择、决策树的生成和决策树的修剪**。  

# 决策树模型与学习

## 决策树模型

**定义：** 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点(node)和有向边(directed edge)组成。节点有两种类型：内部节点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。  
用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归的对实例进行测试并分类，直至达到叶节点。最后将实例分到叶结点的类中。  
如图是一个决策树示意图，图中圆和方框分别表示内部结点和叶结点  
![决策树模型](http://images0.cnblogs.com/blog/790160/201508/281727480004448.png)  
 
 ## 决策树学习
 
 决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。    
**决策树学习的损失函数通常是 正则化的极大似然函数** 。  
**决策树学习的目标是 以损失函数为目标函数的最小化** 。    
> 决策树学习的算法通常是一个 **递归的选择最优特征**，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这个特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归的进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即有了明确的类。这就生成一颗决策树。  
以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据可能过拟合。需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体的就是去掉过于细分的叶结点，使其退回到父结点，甚至更高的结点，然后将其改为新的叶结点。  
若特征较多，在决策树学习开始的时候，对 **特征进行选择**，只留下对训练数据有足够分类能力的特征。  
**决策树的生成只考虑局部最优**  
**决策树的剪枝只考虑全局最优**  

# 特征选择

特征选择在于选取**对训练数据具有分类能力**的特征，可以提高决策树学习的效率。通常特征选择的准则是**信息增益或信息增益率**。    
特征选择的划分依据：这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就应该选择这个特征。（将数据集划分为纯度更高，不确定性更小的子集的过程。）    

## 信息增益

**（1）信息熵：** 

在信息论和概率统计中，**熵(entropy)** 是表示随机变量不确定性的度量（纯度）。是用来衡量一个随机变量出现的期望值。如果信息的不确定性越大，熵的值也就越大，出现的各种情况也就越多。
假设随机变量X的可能取值有x<sub>1，</sub>x<sub>2...,</sub>x<sub>n</sub>,对于每一个可能的取值x<sub>i</sub>，其概率 P(X=x<sub>i</sub>) = p<sub>i</sub> , ( i = 1,2, ... , n)  
因此随机变量X的熵：  
![](https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170323162933986-429761079.png)  
由定义可知，熵只依赖于X的分布，而与X的取值无关。  
对于样本集合D来说，随机变量X是样本的类别，即，假设样本有k个类别，每个类别的概率是<img width="27" height="38" alt="" src="https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317131605495-1565634841.png">，其中|C<sub>k</sub>|表示类别k的样本个数，|D|表示样本总数，则对于样本集合D来说熵（经验熵）为：  
<img width="200" height="57" alt="" src="https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317154810807-1726620815.png">  

**（2）条件熵**

假设有随机变量(X,Y)，其联合概率分布为:P(X=x<sub>i</sub>,Y=y<sub>j</sub>)=p<sub>ij</sub>,i=1,2,⋯,n;j=1,2,⋯,m  
则条件熵(H(Y∣X))表示在已知随机变量X的条件下随机变量Y的不确定性，其定义为X在给定条件下Y的条件概率分布的熵对X的数学期望:  
<img alt="" src="https://img-blog.csdn.net/20141214191555257">  
当嫡和条件嫡中的概率由数据估计(特别是极大似然估计)得到时，所对应的嫡与条件嫡分别称为经验熵( empirical entropy)和经验条件嫡(empirical conditional entropy )。  

**信息增益：**

**定义：** 特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即:  
 &emsp;   &emsp; &emsp; &emsp; &emsp;  **g(D,A)=H(D)-H(D|A)**  
划分前样本集合D的熵是一定的 ，entroy(前)，使用某个特征A划分数据集D，计算划分后的数据子集的熵 entroy(后):      
 &emsp; &emsp; &emsp; &emsp;  **信息增益 =  entroy(前) -  entroy(后)**    

信息增益(information gain)表示由于特征A使得对数据集D的分类的不确定性减少的程度。    
根据信息增益准则的特征选择方法是：对训练数据集D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。  
**信息增益算法：**  
<img width="335" height="145"  src="http://images0.cnblogs.com/blog/790160/201508/281728022508401.png" border="0">    

**缺点：**信息增益偏向取值较多的特征

**原因：**当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较 偏向取值较多的特征。

## 信息增益率

