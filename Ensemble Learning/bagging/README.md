# Bagging & RF

由Boosting内容可知，要想获得泛化性能强的集成，集成中的个体学习器应尽可能**相互独立**。而“独立”在现实任务中比较难以做到，不过我们可以设法使基学习器尽可能具有较大的差异。给定一个训练集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器，这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异。然而，为获得好的集成，我们同时希望个体学习器不能太差。如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，那甚至不能进行有效的学习，更不谈确保产生比较好的基学习器了。于是，为了解决这个问题，我们使用**相互有交叠的采样子集**。  

## Bagging

Bagging是 **并行式集成学习** 方法最著名的代表，它基于前面提到过的**自助采样法**（bootstrap sampling）。给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m此随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。初始训练集中越有63.2%的样本出现在采样集中。（有放回的随机采样）

于是，我们可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再集成，这就是Bagging的基本流程。在对预测输出进行结合时，Bagging通常对**分类任务采用简单投票法**，对**回归任务使用简单平均法**。若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。  

* **包外估计（out-of-bag estimate）：**   
&emsp;&emsp;Bagging的自助采样做法为Bagging带来一个优点是：由于每个基学习器只使用了初始训练集中大约63.2%的样本，剩下的约36.8%的样本则可用作验证集来对泛化性能进行“包外估计”。    
从偏差-方差分解的角度看，Bagging主要关注**降低方差（防止过拟合）**，因此它在不剪枝决策树、神经网络等容易受样本扰动的学习器上效用更为明显。  

与标准AdaBoost只适用于二分类任务不同（为处理多分类或回归任务，AdaBoost需进行修改），Bagging能不经修改的用于多分类、回归等任务。  

## 随机森林

随机森林（Random Forest，简称RF）是Bagging的一个扩展变体。其在以决策树作为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了**随机特征选择。** 因此可以概括RF包括四个部分：1、随机选择样本（放回抽样）；2、随机选择特征；3、构建决策树；4、随机森林投票（平均）。  

具体来说，传统决策树在选择划分属性时是在当前结点的属性集合（假定有d个属性）中选择一个最有属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度：若令k=d，则基决策树的构建与传统决策树相同；若令k=1，则是随机选择一个属性用于划分；一般情况下，推荐值k=log <sub>2</sub> d。


在构建决策树的时候，RF的每棵决策树都最大可能的进行生长而不进行剪枝；在对预测输出进行结合时，RF通常对分类问题使用简单投票法，回归任务使用简单平均法。

RF的重要特性是不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计，它可以**在内部进行评估**，也就是说在生成的过程中可以**对误差进行无偏估计**，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行“包外估计”。

**RF和Bagging对比：**   
RF的收敛性与Bagging相似。随机森林的起始性能往往相对较差，特别是在集成中只包含一个基学习器时，这很容易理解，因为通过引入属性扰动，随机森林中个体学习器的性能往往有所降低。然而，随着个体学习器数目的增加，随机森林通常会收敛到更低的泛化误差。值得一提的是，随机森林的训练效率常优于Bagging，因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，在选择划分属性时要对结点的所有属性进行考察，而随机森林使用的“随机型”决策树则只需考察一个属性子集。  

**优缺点：**

随机森林的优点较多，简单总结：1、在数据集上表现良好，相对于其他算法有较大的优势（训练速度、预测准确度）；  
2、能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性(可用于特征选择)；   
3、容易做成并行化方法。   
RF的缺点：在噪声较大的分类或者回归问题上回过拟合。
