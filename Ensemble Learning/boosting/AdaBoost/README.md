AdaBoost是adaptive boosting( **自适应boosting** )的缩写，其运行过程如下：   
```
&emsp;&emsp;训练数据中的每个样本，并赋予其一个权重，这些权重构成了向量D，一开始，这些权重都初始化成**相等值**。首先在训练数据
上训练出一个若分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练中，将会重新调整每个样本的
权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。为了从所有弱分类器中得到最终的分类结果，AdaBoost为
每个分类器都分配了一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。  计算出alpha值之后，可以对权重向量D进行更
新，以使得那些正确分类的样本的权重降低而错分样本的权重升高。AdaBoost算法会不断地重复训练和调整权重的过程，直到训练错误率为0或弱
分类器的数目达到指定值为止。
```
