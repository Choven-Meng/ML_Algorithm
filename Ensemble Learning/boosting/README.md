# Boosting

Boosting, 也称为增强学习或提升法，是一种重要的集成学习技术， 能够将预测精度仅比随机猜度略高的弱学习器增强为预测精度高的强学习器，

&emsp;&emsp;Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本
分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，（通过改变训练样本的概率分布，提高前一轮弱分类器的分类错误的样本权重，降低正确分类的样本权重）然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到预先指定的值T，最终将这T个基学习器进行**加权结合**。在产生单个的基分类器时可用相同的分类算法,也可用不同的分类算法,这些算法一般是不稳定的弱分类算法,如神经网络(BP) ,决策树(C4.5)等。    

&emsp;&emsp;Boosting算法要求基学习器对特定的数据分布进行学习，这一点是通过“重赋权法”（re-weighting）实现的，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重，对无法接受代全样本的基学习算法，则可通过“重采样法”（re-sampling）来处理，即在每一轮学习中，根据样本分布对训练集重新进行采样，再用**重采样**而得到的样本集对基学习器进行训练。一般而言，这两种做法没有显著的优劣差别。不过由于Boosting算法在训练的每一轮都会检查当前生成的基学习器的是否满足基本条件，若不符合则抛弃当前基学习器，并停止学习过程。在此种情况下，初始设置的学习轮数T也许还远未达到，这会导致最后的集成中只包含很少的基学习器而性能不佳。而若采用“重采样法”，则可以获得“重启动”机会以避免训练过程的过早停止，即在抛弃不满足条件的当前基学习器之后，再根据当前分布重新对训练样本进行重采样，再基于新的采样结果重新训练出基学习器，从而使得学习过程可以持续到预设的T轮完成。

而从偏差-方差分解的角度看，Boosting主要关注**降低偏差（避免欠拟合）**，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。

Boosting也是集合了多个决策树，但是Boosting的每棵树是顺序生成的，每一棵树都依赖于前一颗树。**顺序运行会导致运行速度慢**。
