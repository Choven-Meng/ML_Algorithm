
# K近邻算法

K近邻法是一种基本分类回归方法。  
K近邻法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。K近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。K值的选择、距离度量及分类决策规则是k近邻法的三个基本要素。

## 定义

给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。  
K近邻法的特殊情况是k=1的情况，称为最近邻算法。对于输入的实例点（特征向量）x，最近邻法将数据集中与x最近邻点的类作为x的类。

## 模型

K近邻法中，当训练集、距离度量（如欧氏距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一确定。  
特征空间中，对每个训练实例点xi，距离该点比其他点更近的所有点组成一个区域，叫做单元(cell)。每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最近邻法将实例xi的类yi作为其单元所有点的类标记。这样，每个单元的实例点的类别是确定的。

## 距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反应。K近邻模型的特征空间一般是n维实数向量空间Rn。使用的距离是欧氏距离。  
设特征空间$\chi$是n维实数向量空间$R_n$,$x_i{2} , x_j{2} \in \chi$

$$x_i, x_j \in \chi; $$    

$x_i={(x_i^1,x_i^2,\cdots,x_i^n)}^T;x_j={(x_j^1,x_j^2,\cdots,x_j^n)}^T$

$x_i,x_j$的$L_p$距离定义为：

$L_p（x_i,x_j）=\sqrt[p]{\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p )}$

这里p>=1，当p=2时，成为欧氏距离，即

$L_2(x_i,x_j）=\sqrt[2]{(\sum_{l=1}^n|x_i^{(l)} -x_j^{(l)}|^2 }$

当p=1时，成为曼哈顿距离，即

$L_1（x_i,x_j ）= \sum_{(l=1)}^n|x_i^{(l)}-x_j^{(l)} |$

当p=$\infty$时，它是各个坐标距离的最大值，即

$L_\infty（x_i,x_j ）=max|x_i^{(l)}-x_j^{(l)} |$

## k值的选择

如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，**k值的减小就意味着整体模型变得复杂，容易发生过拟合。**  
如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测。优点是可以减少学习的估计误差。缺点是学习的近似误差增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误，**k值的增大就意味着整体的模型变得简单.**  
**在应用中，k值一般取一个比较小的值。通常采用交叉验证法来选取最优的k值。**  

## 分类决策规则
1、简单投票法  
k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。  
多数表决规则等价于经验风险最小化。  

2、加权投票法  
根据距离的远近，对近邻的投票进行加权，距离越近则权重越大（权重为距离平方的倒数）

## KNN算法实现　

算法基本步骤：

1）计算待分类点与已知类别的点之间的距离  
2）按照距离递增次序排序  
3）选取与待分类点距离最小的k个点    
4）确定前k个点所在类别的出现次数  
5）返回前k个点出现次数最高的类别作为待分类点的预测分类  

### KNN算法的缺陷

   该算法在分类时有个重要的不足是，当样本不平衡时，即：一个类的样本容量很大，而其他类样本数量很小时，很有可能导致当输入一个未知样本时，该样本的K个邻居中大数量类的样本占多数。 但是这类样本并不接近目标样本，而数量小的这类样本很靠近目标样本。这个时候，我们有理由认为该位置样本属于数量小的样本所属的一类，但是，KNN却不关心这个问题，它只关心哪类样本的数量最多，而不去把距离远近考虑在内，因此，我们可以采用权值的方法来改进。和该样本距离小的邻居权值大，和该样本距离大的邻居权值则相对较小，由此，将距离远近的因素也考虑在内，避免因一个样本过大导致误判的情况。

　　从算法实现的过程可以发现，该算法存两个严重的问题，第一个是需要存储全部的训练样本，第二个是计算量较大，因为对每一个待分类的样本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。KNN算法的改进方法之一是分组快速搜索近邻法。其基本思想是：将样本集按近邻关系分解成组，给出每组质心的位置，以质心作为代表点，和未知样本计算距离，选出距离最近的一个或若干个组，再在组的范围内应用一般的KNN算法。由于并不是将未知样本与所有样本计算距离，故该改进算法可以减少计算量，但并不能减少存储量。

## k近邻法的实现：kd树

实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。在特征空间的维数大及训练数据容量大时尤其必要。  
k近邻法最简单的实现方法是 **线性扫描**， 要计算输入实例与每个训练实例的距离。但当训练数据很大时，耗时不可取。  
为了提高k紧邻搜索的效率，可以用特殊的结构存储训练数据，以减少计算距离的次数，如kd树方法。

### 构造kd树

kd树是存储k维空间数据的树结构，是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分，构造kd树相当于不断的用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个节点对应于一个k维超矩形区域。

![](https://images2015.cnblogs.com/blog/890966/201611/890966-20161123110856159-175985324.png)

> **构造kd树的方法如下：** 
> 构造根结点，使根结点对应于K维空间中包含所有实例点的超矩形区域；通过下面的递归的方法，不断地对k维空间进行切分，生成子结点。在超矩形区域上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域，这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。通常，循环的择坐标轴对空间切分，选择训练实例点在坐标轴上的中位数为切分点，这样得到的kd树是平衡的（平衡二叉树：它是一棵空树，或其左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是平衡二叉树）。 **注意，平衡的kd树搜索时未必是最优的**  

**下面用一个简单的2维平面上的例子来进行说明。**

> 例. 给定一个二维空间数据集：T={(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}构造一个平衡kd树。  
> 解：根结点对应包含数据集T的矩形，选择x^(1)轴，6个数据点的x^(1)坐标中位数是7，以平面x^(1)=7将空间分为左、右两个子矩形（子结点）；接着左矩形以x^(2)=4分为两个子矩形（左矩形中{(2,3),(5,4),(4,7)}点的x^(2)坐标中位数正好为4），右矩形以x^(2)=6分为两个子矩形，如此递归，最后得到如下图所示的特征空间划分和kd树。

![](https://images2015.cnblogs.com/blog/890966/201611/890966-20161123134503362-571302342.png)

### 搜索kd树

利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。下面以搜索最近邻点为例加以叙述：给定一个目标点，搜索其最近邻，首先找到包含目标点的叶节点；然后从该叶结点出发，依次回退到父结点；不断查找与目标点最近邻的结点，当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提高。  

**用kd树的最近邻搜索：**　　

输入： 已构造的kd树；目标点x；   
输出：x的最近邻。  

（1） 在kd树中找出包含目标点x的叶结点：从根结点出发，递归的向下访问kd树。若目标点当前维的坐标值小于切分点的坐标值，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止；  
（2） 以此叶结点为“当前最近点”；  
（3） 递归的向上回退，在每个结点进行以下操作：  
 *    （a） 如果该结点保存的实例点比当前最近点距目标点更近，则以该实例点为“当前最近点”；  
 *    （b） 当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一个子结点对应的区域是否有更近的点。具体的，检查另一个子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距离目标更近的点，移动到另一个子结点。接着，递归的进行最近邻搜索。如果不相交，向上回退。          
（4） 当回退到根结点时，搜索结束。最后的“当前最近点”即为x的最近邻点。  

> 以先前构建好的kd树为例，查找目标点（3,4.5）的最近邻点。同样先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径：（7,2）→（5,4）→（4,7），取（4,7）为当前最近邻点。以目标查找点为圆心，目标查找点到当前最近点的距离2.69为半径确定一个红色的圆。然后回溯到（5,4），计算其与查找点之间的距离为2.06，则该结点比当前最近点距目标点更近，以(5,4)为当前最近点。用同样的方法再次确定一个绿色的圆，可见该圆和y = 4超平面相交，所以需要进入（5,4）结点的另一个子空间进行查找。（2,3）结点与目标点距离为1.8，比当前最近点要更近，所以最近邻点更新为（2，3），最近距离更新为1.8，同样可以确定一个蓝色的圆。接着根据规则回退到根结点(7,2)，蓝色圆与x=7的超平面不相交，因此不用进入（7,2）的右子空间进行查找。至此，搜索路径回溯完，返回最近邻点（2,3），最近距离1.8。  
![](https://images2015.cnblogs.com/blog/890966/201611/890966-20161123150431143-1520224794.png)




**问题一：Kd-Tree与一维二叉查找树之间的区别：**

二叉查找树：数据存放在树中的每个结点（根结点、中间结点、叶子结点）中；  
Kd-Tree：数据只存放在叶子结点，而根结点和中间结点存放一些空间划分信息（例如划分维度、划分值）；  

**问题二：* 每次对子空间的划分时，怎样确定在哪个维度上进行划分？*

最简单的方法就是轮着来，即如果这次选择了在第i维上进行数据划分，那下一次就在第j(j≠i)维上进行划分，例如：j = (i mod k) + 1。（对比切豆腐）

最大方差法（max invarince），即每次我们选择维度进行划分时，都选择具有最大方差维度。（对比切细木条）


# 扩展：Kd-tree with BBF
