
# K近邻算法

K近邻法是一种基本分类回归方法。  
K近邻法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。K近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。K值的选择、距离度量及分类决策规则是k近邻法的三个基本要素。

## 定义

给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。  
K近邻法的特殊情况是k=1的情况，称为最近邻算法。对于输入的实例点（特征向量）x，最近邻法将数据集中与x最近邻点的类作为x的类。

## 模型

K近邻法中，当训练集、距离度量（如欧氏距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一确定。  
特征空间中，对每个训练实例点xi，距离该点比其他点更近的所有点组成一个区域，叫做单元(cell)。每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最近邻法将实例xi的类yi作为其单元所有点的类标记。这样，每个单元的实例点的类别是确定的。

## 距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反应。K近邻模型的特征空间一般是n维实数向量空间Rn。使用的距离是欧氏距离。  
设特征空间$\chi$是n维实数向量空间$R_n$,$x_i{2} , x_j{2} \in \chi$

$$x_i, x_j \in \chi; $$    

$x_i={(x_i^1,x_i^2,\cdots,x_i^n)}^T;x_j={(x_j^1,x_j^2,\cdots,x_j^n)}^T$

$x_i,x_j$的$L_p$距离定义为：

$L_p（x_i,x_j）=\sqrt[p]{\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p )}$

这里p>=1，当p=2时，成为欧氏距离，即

$L_2(x_i,x_j）=\sqrt[2]{(\sum_{l=1}^n|x_i^{(l)} -x_j^{(l)}|^2 }$

当p=1时，成为曼哈顿距离，即

$L_1（x_i,x_j ）= \sum_{(l=1)}^n|x_i^{(l)}-x_j^{(l)} |$

当p=$\infty$时，它是各个坐标距离的最大值，即

$L_\infty（x_i,x_j ）=max|x_i^{(l)}-x_j^{(l)} |$

## k值的选择

如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，**k值的减小就意味着整体模型变得复杂，容易发生过拟合。**  
如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测。优点是可以减少学习的估计误差。缺点是学习的近似误差增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误，**k值的增大就意味着整体的模型变得简单.**  
**在应用中，k值一般取一个比较小的值。通常采用交叉验证法来选取最优的k值。**  

## 分类决策规则

k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。  
多数表决规则等价于经验风险最小化。  

## k近邻法的实现：kd树

实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。在特征空间的维数大及训练数据容量大时尤其必要。  
k近邻法最简单的实现方法是 **线性扫描**， 要计算输入实例与每个训练实例的距离。但当训练数据很大时，耗时不可取。  
为了提高k紧邻搜索的效率，可以用特殊的结构存储训练数据，以减少计算距离的次数，如kd树方法。

### 构造kd树

kd树是存储k维空间数据的树结构，是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分，构造kd树相当于不断的用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个节点对应于一个k维超矩形区域。
![](https://images2015.cnblogs.com/blog/890966/201611/890966-20161123110856159-175985324.png)
> **构造kd树的方法如下：** 构造根结点，使根结点对应于K维空间中包含所有实例点的超矩形区域；通过下面的递归的方法，不断地对k维空间进行切分，生成子结点。在超矩形区域上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域，这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。通常，循环的择坐标轴对空间切分，选择训练实例点在坐标轴上的中位数为切分点，这样得到的kd树是平衡的（平衡二叉树：它是一棵空树，或其左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是平衡二叉树）。 **注意，平衡的kd树搜索时未必是最优的**  

**下面用一个简单的2维平面上的例子来进行说明。**

> 例. 给定一个二维空间数据集：T={(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}构造一个平衡kd树。  
> 解：根结点对应包含数据集T的矩形，选择x^(1)轴，6个数据点的x^(1)坐标中位数是6，这里选最接近的(7,2)点，以平面x^(1)=7将空间分为左、右两个子矩形（子结点）；接着左矩形以x^(2)=4分为两个子矩形（左矩形中{(2,3),(5,4),(4,7)}点的x^(2)坐标中位数正好为4），右矩形以x^(2)=6分为两个子矩形，如此递归，最后得到如下图所示的特征空间划分和kd树。
![](https://images2015.cnblogs.com/blog/890966/201611/890966-20161123134503362-571302342.png)



**Kd-Tree与一维二叉查找树之间的区别：**

二叉查找树：数据存放在树中的每个结点（根结点、中间结点、叶子结点）中；  
Kd-Tree：数据只存放在叶子结点，而根结点和中间结点存放一些空间划分信息（例如划分维度、划分值）；  



![](https://gss2.bdstatic.com/9fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D220/sign=1460bee8b9389b503cffe750b534e5f1/838ba61ea8d3fd1f638506cc304e251f94ca5fb3.jpg)

