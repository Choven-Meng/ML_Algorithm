
# K近邻算法


K近邻法是一种基本分类回归方法。

K近邻法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。K近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。K值的选择、距离度量及分类决策规则是k近邻法的三个基本要素。

## 定义

给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。

K近邻法的特殊情况是k=1的情况，称为最近邻算法。对于输入的实例点（特征向量）x，最近邻法将数据集中与x最近邻点的类作为x的类。

## 模型

K近邻法中，当训练集、距离度量（如欧氏距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一确定。

特征空间中，对每个训练实例点xi，距离该点比其他点更近的所有点组成一个区域，叫做单元(cell)。每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最近邻法将实例xi的类yi作为其单元所有点的类标记。这样，每个单元的实例点的类别是确定的。

## 距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反应。K近邻模型的特征空间一般是n维实数向量空间Rn。使用的距离是欧氏距离。

设特征空间χ是n维实数向量空间Rn，

x_i,x_j∈χ,x_i=(x_i^((1) ),x_i^((2) ),…x_i^((n) ) )^T,x_j=(x_j^((1) ),x_j^((2) ),…x_j^((n) ) )^T

x_i,x_j的L_p距离定义为：

L_p （x_i,x_j ）=〖(∑_(l=1)^n▒〖|x_i^((l) )-x_j^((l) ) |〗^p )〗^(1/p)

这里p>=1，当p=2时，成为欧氏距离，即

L_2 （x_i,x_j ）=〖(∑_(l=1)^n▒〖|x_i^((l) )-x_j^((l) ) |〗^2 )〗^(1/2)

当p=1时，成为曼哈顿距离，即

L_1 （x_i,x_j ）=∑_(l=1)^n▒〖|x_i^((l) )-x_j^((l) ) |〗

当p=∞时，它是各个坐标距离的最大值，即

L_∞ （x_i,x_j ）=max|x_i^((l) )-x_j^((l) ) |
## k值的选择

如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k值的减小就意味着整体模型变得复杂，容易发生过拟合。
如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测。优点是可以减少学习的估计误差。缺点是学习的近似误差增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误，k值的增大就意味着整体的模型变得简单.
在应用中，k值一般取一个比较小的值。通常采用交叉验证法来选取最优的k值。
