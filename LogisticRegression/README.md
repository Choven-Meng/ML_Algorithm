&emsp;&emsp;Logistic回归属于分类模型。回顾线性回归，输出的是连续的实数，而Logistic回归输出的是[0,1]区间的概率值，通过概率值来判断因变量应该是1还0(简单来说，它的结果不是1就是0)。因此，虽然名字中带着“回归”（输出范围常为连续实数），但Logistic回归属于分类模型（输出范围为一组离散值构成的集合）。  
&emsp;&emsp;最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足
约束条件的模型集合中选取熵最大的模型。  
 **当X服从均匀分布时，熵最大。**  
 &emsp;&emsp;最大熵模型的学习过程就是求解最大熵模型的过程。最大熵模型的学习可以形式化为约束最优化问题。  
 
 ### 模型学习的最优化方法
 
 &emsp;&emsp;logistic回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过 **迭代算法** 求解。  
 常见的优化方法有改进的 **迭代尺度法(IIS)、梯度下降法、牛顿法或拟牛顿法**。牛顿法或拟牛顿法一般收敛速度更快。

## logistic回归是如何工作的

Logistic 回归通过使用其固有的 logistic 函数估计概率，来衡量因变量（我们想要预测的标签）与一个或多个自变量（特征）之间的关系。   
然后这些概率必须二值化才能真地进行预测。这就是 logistic 函数的任务，也称为 sigmoid 函数。Sigmoid 函数是一个 S 形曲线，它可以将任意实数值映射到介于 0 和 1 之间的值，但并不会取到 0/1。然后使用阈值分类器将 0 和 1 之间的值转换为 0 或 1。sigmoid 函数:  

<img src="https://img-blog.csdn.net/20170426142819918" alt="">

下面的图片说明了 logistic 回归得出预测所需的所有步骤：  
<img  src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicMpaiaku5HZVfmoEL2hmBBficJdw7lyMLr99aG0fzOg6O28Z1NTeBdZAg8twVbgHrcTQ8qZtjLqfaQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" width=400 height=256  >  


下面是 logistic 函数（sigmoid 函数）的图形表示：

<img src="https://img-blog.csdn.net/20170426143451022" alt="">

当z为0的时候，函数值为0.5；随着z的增大，函数值逼近于1；随着z的减小，函数值逼近于0.  
所以，这个函数很适合做我们刚才提到的二分类的分类函数。假设输入数据的特征是(x0, x1, x2, ..., xn)，我们在每个特征上乘以一个回归系数 (w0, w1, w2, ... , wn)，然后累加得到sigmoid函数的输入z：  
<img src="https://img-blog.csdn.net/20170426144754539" alt="">  
那么，输出就是一个在0~1之间的值，我们把输出大于0.5的数据分到1类，把输出小于0.5的数据分到0类。这就是Logistic回归的分类过程.  
我们要做的，就是确定这个分类器中的 **最佳回归系数**，即(w0, w1, w2, ... , wn)  



我们希望随机数据点 **被正确分类的概率最大化**，这就是 **最大似然估计**。最大似然估计是统计模型中估计参数的通用方法。  
你可以使用不同的方法（如优化算法）来最大化概率。牛顿法也是其中一种，可用于查找许多不同函数的最大值（或最小值），包括似然函数。也可以用梯度下降法代替牛顿法。  

## 求解规划模型

### 1、最小二乘法

最小二乘法通过数学推导得到全局最优解的表达式，是一种完全数学描述的方法，直接给出求解公式。   
最小二乘法可以得到全局最优解，但是因涉及超大矩阵的求逆运算而难以求解。  
<img src="https://images2015.cnblogs.com/blog/793413/201701/793413-20170124135408253-1935358158.png">  

### 2、梯度下降（上升）法

梯度下降法是一种典型的贪心算法，它从任意一组参数开始，向着使 **目标函数最小** 的方向调整参数，直至无法使目标函数继续下降时，停止计算。   
多元函数微积分中, 梯度指向函数值变化最快方向的向量. 梯度下降法 **无法保证** 的得到全局最优解。   
梯度下降法有批量梯度下降法和随机梯度下降法两种实现方法。  

#### 2.1、批量梯度下降（上升）法 （Batch Gradient Descent/Ascent）

算法流程：
```
初始化回归系数为1
重复执行直至收敛 ｛
    计算整个数据集的梯度
    按照递推公式更新回归梯度
｝
返回最优回归系数值
```  
将损失函数J(w)求偏导，得到J(w)的梯度。以矩阵形式给出：  
<img src="https://images2015.cnblogs.com/blog/793413/201701/793413-20170124135425284-1058242784.png">  
alpha是下降步长，由迭代公式：  
<img src="https://images2015.cnblogs.com/blog/793413/201701/793413-20170124135436847-2014812374.png">  

#### 2.2、随机梯度下降（上升）法（stochastic gradient Descent/Ascent）

算法流程：  
```
初始化回归系数为1
重复执行直至收敛 ｛
    对每一个训练样本｛
        计算样本的梯度
        按照递推公式更新回归梯度
    ｝
｝
返回最优回归系数值
```  
为了加快收敛速度，做出两个改进：   
(1)在每次迭代时，调整更新步长alpha的值。随着迭代的进行，alpha越来越小   
(2)每次迭代改变样本的顺序,也就是随机选择样本来更新回归系数


## Logistic 回归 vs 线性回归 

&emsp;&emsp;逻辑回归得到一个离散的结果，但线性回归得到一个连续的结果。预测房价的模型算是返回连续结果的一个好例子。该值根据房子大小或位置等参数的变化而变化。离散的结果总是一件事（有）或另一个（没有）。  

## 优缺点

**优点：**  
> Logistic 回归是一种被人们广泛使用的算法，因为它非常高效，不需要太大的计算量，又通俗易懂，不需要缩放输入特征，不需要任何调整，且很容易调整，并且输出校准好的预测概率。   
> 与线性回归一样，当去掉与输出变量无关的属性以及相似度高的属性时，logistic 回归效果确实会更好。因此特征处理在 Logistic 和线性回归的性能方面起着重要的作用。  
> Logistic 回归的另一个优点是它非常容易实现，且训练起来很高效。在研究中，我通常以 Logistic 回归模型作为基准，再尝试使用更复杂的算法。  

 **缺点：**  
 > 我们不能用 logistic 回归来解决非线性问题. 
 >  容易欠拟合，分类精度可能不高。  
 > 高度依赖正确的数据表示。  
 
 #### 适用场景  
 
 Logistic 回归通过线性边界将输入分成两个「区域」，每个类别划分一个区域。因此，数据应当是线性可分的，如下图所示的数据点：  
 <img src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicMpaiaku5HZVfmoEL2hmBBfSkTCOWcjBoUtZe0h2GicnDXNpur8F7l0EMuYDibiaEDXU6s4AF2Sljsxw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" width=350 height=256>  
 
 换句话说：当 Y 变量只有 **两个值** 时（例如，面临分类问题时），应该考虑使用逻辑回归。注意，也可以将 Logistic 回归用于多类别分类。
 
 
 
 
